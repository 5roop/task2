{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at classla/bcms-bertic were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15491\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m5roop\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">./outputs</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/5roop/huggingface\" target=\"_blank\">https://wandb.ai/5roop/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/5roop/huggingface/runs/1coyltmq\" target=\"_blank\">https://wandb.ai/5roop/huggingface/runs/1coyltmq</a><br/>\n",
       "                Run data is saved locally in <code>/home/peterr/macocu/task1/task2/wandb/run-20210824_145244-1coyltmq</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15491' max='15491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15491/15491 38:18, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.586100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.530500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.552200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.453400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.428000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.099300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11500\n",
      "Configuration saved in ./outputs/checkpoint-11500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-12000\n",
      "Configuration saved in ./outputs/checkpoint-12000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-12500\n",
      "Configuration saved in ./outputs/checkpoint-12500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-12500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-13000\n",
      "Configuration saved in ./outputs/checkpoint-13000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-13500\n",
      "Configuration saved in ./outputs/checkpoint-13500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-14000\n",
      "Configuration saved in ./outputs/checkpoint-14000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-14500\n",
      "Configuration saved in ./outputs/checkpoint-14500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-15000\n",
      "Configuration saved in ./outputs/checkpoint-15000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-15000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15491, training_loss=0.2286161774008664, metrics={'train_runtime': 2304.3218, 'train_samples_per_second': 26.887, 'train_steps_per_second': 6.723, 'total_flos': 1.630157165693952e+16, 'train_loss': 0.2286161774008664, 'epoch': 7.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ElectraTokenizerFast, ElectraForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "def read_file(fname: str, correct_labels=False) -> pd.DataFrame:\n",
    "    \"\"\"Reads a filename, return df with text and labels.\n",
    "\n",
    "    Args:\n",
    "        fname (str): Filename to read\n",
    "        correct_labels (bool, optional): If True, offensive instances get labeled 1\n",
    "        and acceptable speech gets labeled 0. Else the labels remain unchanged.\n",
    "        Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: resulting dataframe with columns: text, labels\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_table(fname, sep=\"\\t\", header=None, names=\"text,labels,role\".split(\",\"))\n",
    "    if correct_labels:\n",
    "        offensive_ids = df.labels != \"Acceptable speech\"\n",
    "        df.labels[offensive_ids] = 1\n",
    "        df.labels[~offensive_ids] = 0\n",
    "        df[\"labels\"] = df.labels.astype(int)\n",
    "    df = df.drop(columns=[\"role\"])\n",
    "    return df\n",
    "\n",
    "en_test, en_train = \"../data/merged-en.test.tsv\" , \"../data/merged-en.train.tsv\"\n",
    "hr_test, hr_train = \"../data/merged-hr.test.tsv\" , \"../data/merged-hr.train.tsv\"\n",
    "sl_test, sl_train = \"../data/merged-sl.test.tsv\",  \"../data/merged-sl.train.tsv\"\n",
    "\n",
    "model_name = \"classla/bcms-bertic\"\n",
    "\n",
    "train_df = read_file(hr_train, correct_labels=True)\n",
    "test_df = read_file(hr_test, correct_labels=True)\n",
    "\n",
    "train_texts, train_labels = train_df.text.values.tolist(), train_df.labels.values.tolist()\n",
    "test_texts, test_labels = test_df.text.values.tolist(), test_df.labels.values.tolist()\n",
    "\n",
    "class MergedHateDataset(Dataset):\n",
    "    \"\"\" A dataset class for the merged hatespeech dataset (Frank)\n",
    "    \"\"\"    \n",
    "    def __init__(self, encodings, labels) -> None:\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[index])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = MergedHateDataset(train_encodings, train_labels)\n",
    "test_dataset = MergedHateDataset(test_encodings, test_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./outputs\",\n",
    "    num_train_epochs = 7,\n",
    "    per_device_train_batch_size = 4,\n",
    "    warmup_steps = 100,\n",
    "    learning_rate = 3e-5,\n",
    "    logging_dir = \"./runs\",\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n",
    "model = ElectraForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b0bff7e381e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"finetuned_models/HR_hate___classla_bcms-bertic_5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "out_filename = \"finetuned_models/HR_hate___classla_bcms-bertic_5\"\n",
    "model.save_pretrained(out_filename)\n",
    "tokenizer.save_pretrained(out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeated training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file finetuned_models/HR_hate___classla_bcms-bertic_10/added_tokens.json. We won't load it.\n",
      "loading file finetuned_models/HR_hate___classla_bcms-bertic_10/vocab.txt\n",
      "loading file finetuned_models/HR_hate___classla_bcms-bertic_10/tokenizer.json\n",
      "loading file None\n",
      "loading file finetuned_models/HR_hate___classla_bcms-bertic_10/special_tokens_map.json\n",
      "loading file finetuned_models/HR_hate___classla_bcms-bertic_10/tokenizer_config.json\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file finetuned_models/HR_hate___classla_bcms-bertic_10/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"finetuned_models/HR_hate___classla_bcms-bertic_9\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file finetuned_models/HR_hate___classla_bcms-bertic_10/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at finetuned_models/HR_hate___classla_bcms-bertic_10.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='11065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/11065 27:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.014900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11065, training_loss=0.020669079484090618, metrics={'train_runtime': 1631.2458, 'train_samples_per_second': 27.13, 'train_steps_per_second': 6.783, 'total_flos': 1.16439797549568e+16, 'train_loss': 0.020669079484090618, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ElectraTokenizerFast, ElectraForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "in_filename = \"finetuned_models/HR_hate___classla_bcms-bertic_10\"\n",
    "\n",
    "def read_file(fname: str, correct_labels=False) -> pd.DataFrame:\n",
    "    \"\"\"Reads a filename, return df with text and labels.\n",
    "\n",
    "    Args:\n",
    "        fname (str): Filename to read\n",
    "        correct_labels (bool, optional): If True, offensive instances get labeled 1\n",
    "        and acceptable speech gets labeled 0. Else the labels remain unchanged.\n",
    "        Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: resulting dataframe with columns: text, labels\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_table(fname, sep=\"\\t\", header=None, names=\"text,labels,role\".split(\",\"))\n",
    "    if correct_labels:\n",
    "        offensive_ids = df.labels != \"Acceptable speech\"\n",
    "        df.labels[offensive_ids] = 1\n",
    "        df.labels[~offensive_ids] = 0\n",
    "        df[\"labels\"] = df.labels.astype(int)\n",
    "    df = df.drop(columns=[\"role\"])\n",
    "    return df\n",
    "\n",
    "en_test, en_train = \"../data/merged-en.test.tsv\" , \"../data/merged-en.train.tsv\"\n",
    "hr_test, hr_train = \"../data/merged-hr.test.tsv\" , \"../data/merged-hr.train.tsv\"\n",
    "sl_test, sl_train = \"../data/merged-sl.test.tsv\",  \"../data/merged-sl.train.tsv\"\n",
    "\n",
    "model_name = in_filename\n",
    "\n",
    "train_df = read_file(hr_train, correct_labels=True)\n",
    "test_df = read_file(hr_test, correct_labels=True)\n",
    "\n",
    "train_texts, train_labels = train_df.text.values.tolist(), train_df.labels.values.tolist()\n",
    "test_texts, test_labels = test_df.text.values.tolist(), test_df.labels.values.tolist()\n",
    "\n",
    "class MergedHateDataset(Dataset):\n",
    "    \"\"\" A dataset class for the merged hatespeech dataset (Frank)\n",
    "    \"\"\"    \n",
    "    def __init__(self, encodings, labels) -> None:\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[index])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = MergedHateDataset(train_encodings, train_labels)\n",
    "test_dataset = MergedHateDataset(test_encodings, test_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./outputs\",\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 4,\n",
    "    warmup_steps = 100,\n",
    "    learning_rate = 3e-5,\n",
    "    logging_dir = \"./runs\",\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n",
    "model = ElectraForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in finetuned_models/HR_hate___classla_bcms-bertic_11/config.json\n",
      "Model weights saved in finetuned_models/HR_hate___classla_bcms-bertic_11/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_models/HR_hate___classla_bcms-bertic_11/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_models/HR_hate___classla_bcms-bertic_11/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('finetuned_models/HR_hate___classla_bcms-bertic_11/tokenizer_config.json',\n",
       " 'finetuned_models/HR_hate___classla_bcms-bertic_11/special_tokens_map.json',\n",
       " 'finetuned_models/HR_hate___classla_bcms-bertic_11/vocab.txt',\n",
       " 'finetuned_models/HR_hate___classla_bcms-bertic_11/added_tokens.json',\n",
       " 'finetuned_models/HR_hate___classla_bcms-bertic_11/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_filename_after_additional_training = \"finetuned_models/HR_hate___classla_bcms-bertic_11\"\n",
    "model.save_pretrained(out_filename_after_additional_training)\n",
    "tokenizer.save_pretrained(out_filename_after_additional_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDIA/crosloengual-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/vocab.txt from cache at /home/peterr/.cache/huggingface/transformers/09f73beefae6e0412fcfe3cc0d7cdd26efc944b005bfc2a9578ddbe1236ff2b5.32b339c0808458d0322a520d66b9be44f71818893fd19ec5a2e21e19799521ed\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer_config.json from cache at /home/peterr/.cache/huggingface/transformers/6df9a50970eafe70edbc4e0098fbe8781b666e868e9b9b36ceb9890bb7fcf0bf.268a1e35e9054a0c7fb4bc185891f11122ab5e15883e3333534cdf3d76681112\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5eb6f38d605ba2a329b257215c4d2ca7f74914bc1f224ffd4f01bc005e47bd28.e1011c59a7da068c6b0be79e739ecac88c0e551e50f89d8025654e94ef118633\n",
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='11065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/11065 27:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.667100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.543600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.564500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.479300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.429100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/config.json\n",
      "Model weights saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/special_tokens_map.json\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/vocab.txt from cache at /home/peterr/.cache/huggingface/transformers/09f73beefae6e0412fcfe3cc0d7cdd26efc944b005bfc2a9578ddbe1236ff2b5.32b339c0808458d0322a520d66b9be44f71818893fd19ec5a2e21e19799521ed\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer_config.json from cache at /home/peterr/.cache/huggingface/transformers/6df9a50970eafe70edbc4e0098fbe8781b666e868e9b9b36ceb9890bb7fcf0bf.268a1e35e9054a0c7fb4bc185891f11122ab5e15883e3333534cdf3d76681112\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5eb6f38d605ba2a329b257215c4d2ca7f74914bc1f224ffd4f01bc005e47bd28.e1011c59a7da068c6b0be79e739ecac88c0e551e50f89d8025654e94ef118633\n",
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='11065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/11065 28:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/config.json\n",
      "Model weights saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/special_tokens_map.json\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/vocab.txt from cache at /home/peterr/.cache/huggingface/transformers/09f73beefae6e0412fcfe3cc0d7cdd26efc944b005bfc2a9578ddbe1236ff2b5.32b339c0808458d0322a520d66b9be44f71818893fd19ec5a2e21e19799521ed\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer_config.json from cache at /home/peterr/.cache/huggingface/transformers/6df9a50970eafe70edbc4e0098fbe8781b666e868e9b9b36ceb9890bb7fcf0bf.268a1e35e9054a0c7fb4bc185891f11122ab5e15883e3333534cdf3d76681112\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5eb6f38d605ba2a329b257215c4d2ca7f74914bc1f224ffd4f01bc005e47bd28.e1011c59a7da068c6b0be79e739ecac88c0e551e50f89d8025654e94ef118633\n",
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='11065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/11065 28:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/config.json\n",
      "Model weights saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/special_tokens_map.json\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/vocab.txt from cache at /home/peterr/.cache/huggingface/transformers/09f73beefae6e0412fcfe3cc0d7cdd26efc944b005bfc2a9578ddbe1236ff2b5.32b339c0808458d0322a520d66b9be44f71818893fd19ec5a2e21e19799521ed\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer_config.json from cache at /home/peterr/.cache/huggingface/transformers/6df9a50970eafe70edbc4e0098fbe8781b666e868e9b9b36ceb9890bb7fcf0bf.268a1e35e9054a0c7fb4bc185891f11122ab5e15883e3333534cdf3d76681112\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5eb6f38d605ba2a329b257215c4d2ca7f74914bc1f224ffd4f01bc005e47bd28.e1011c59a7da068c6b0be79e739ecac88c0e551e50f89d8025654e94ef118633\n",
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='11065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/11065 28:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/config.json\n",
      "Model weights saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/special_tokens_map.json\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/vocab.txt from cache at /home/peterr/.cache/huggingface/transformers/09f73beefae6e0412fcfe3cc0d7cdd26efc944b005bfc2a9578ddbe1236ff2b5.32b339c0808458d0322a520d66b9be44f71818893fd19ec5a2e21e19799521ed\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer_config.json from cache at /home/peterr/.cache/huggingface/transformers/6df9a50970eafe70edbc4e0098fbe8781b666e868e9b9b36ceb9890bb7fcf0bf.268a1e35e9054a0c7fb4bc185891f11122ab5e15883e3333534cdf3d76681112\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5eb6f38d605ba2a329b257215c4d2ca7f74914bc1f224ffd4f01bc005e47bd28.e1011c59a7da068c6b0be79e739ecac88c0e551e50f89d8025654e94ef118633\n",
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='11065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/11065 28:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/config.json\n",
      "Model weights saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/special_tokens_map.json\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/vocab.txt from cache at /home/peterr/.cache/huggingface/transformers/09f73beefae6e0412fcfe3cc0d7cdd26efc944b005bfc2a9578ddbe1236ff2b5.32b339c0808458d0322a520d66b9be44f71818893fd19ec5a2e21e19799521ed\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer_config.json from cache at /home/peterr/.cache/huggingface/transformers/6df9a50970eafe70edbc4e0098fbe8781b666e868e9b9b36ceb9890bb7fcf0bf.268a1e35e9054a0c7fb4bc185891f11122ab5e15883e3333534cdf3d76681112\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5eb6f38d605ba2a329b257215c4d2ca7f74914bc1f224ffd4f01bc005e47bd28.e1011c59a7da068c6b0be79e739ecac88c0e551e50f89d8025654e94ef118633\n",
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='11065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/11065 28:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/config.json\n",
      "Model weights saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/special_tokens_map.json\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/vocab.txt from cache at /home/peterr/.cache/huggingface/transformers/09f73beefae6e0412fcfe3cc0d7cdd26efc944b005bfc2a9578ddbe1236ff2b5.32b339c0808458d0322a520d66b9be44f71818893fd19ec5a2e21e19799521ed\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer_config.json from cache at /home/peterr/.cache/huggingface/transformers/6df9a50970eafe70edbc4e0098fbe8781b666e868e9b9b36ceb9890bb7fcf0bf.268a1e35e9054a0c7fb4bc185891f11122ab5e15883e3333534cdf3d76681112\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5eb6f38d605ba2a329b257215c4d2ca7f74914bc1f224ffd4f01bc005e47bd28.e1011c59a7da068c6b0be79e739ecac88c0e551e50f89d8025654e94ef118633\n",
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='11065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/11065 28:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/config.json\n",
      "Model weights saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/special_tokens_map.json\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/vocab.txt from cache at /home/peterr/.cache/huggingface/transformers/09f73beefae6e0412fcfe3cc0d7cdd26efc944b005bfc2a9578ddbe1236ff2b5.32b339c0808458d0322a520d66b9be44f71818893fd19ec5a2e21e19799521ed\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer_config.json from cache at /home/peterr/.cache/huggingface/transformers/6df9a50970eafe70edbc4e0098fbe8781b666e868e9b9b36ceb9890bb7fcf0bf.268a1e35e9054a0c7fb4bc185891f11122ab5e15883e3333534cdf3d76681112\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5eb6f38d605ba2a329b257215c4d2ca7f74914bc1f224ffd4f01bc005e47bd28.e1011c59a7da068c6b0be79e739ecac88c0e551e50f89d8025654e94ef118633\n",
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='11065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/11065 28:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/config.json\n",
      "Model weights saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/special_tokens_map.json\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/vocab.txt from cache at /home/peterr/.cache/huggingface/transformers/09f73beefae6e0412fcfe3cc0d7cdd26efc944b005bfc2a9578ddbe1236ff2b5.32b339c0808458d0322a520d66b9be44f71818893fd19ec5a2e21e19799521ed\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer_config.json from cache at /home/peterr/.cache/huggingface/transformers/6df9a50970eafe70edbc4e0098fbe8781b666e868e9b9b36ceb9890bb7fcf0bf.268a1e35e9054a0c7fb4bc185891f11122ab5e15883e3333534cdf3d76681112\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5eb6f38d605ba2a329b257215c4d2ca7f74914bc1f224ffd4f01bc005e47bd28.e1011c59a7da068c6b0be79e739ecac88c0e551e50f89d8025654e94ef118633\n",
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='11065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/11065 28:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/config.json\n",
      "Model weights saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/special_tokens_map.json\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/vocab.txt from cache at /home/peterr/.cache/huggingface/transformers/09f73beefae6e0412fcfe3cc0d7cdd26efc944b005bfc2a9578ddbe1236ff2b5.32b339c0808458d0322a520d66b9be44f71818893fd19ec5a2e21e19799521ed\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer_config.json from cache at /home/peterr/.cache/huggingface/transformers/6df9a50970eafe70edbc4e0098fbe8781b666e868e9b9b36ceb9890bb7fcf0bf.268a1e35e9054a0c7fb4bc185891f11122ab5e15883e3333534cdf3d76681112\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5eb6f38d605ba2a329b257215c4d2ca7f74914bc1f224ffd4f01bc005e47bd28.e1011c59a7da068c6b0be79e739ecac88c0e551e50f89d8025654e94ef118633\n",
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11065' max='11065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11065/11065 28:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-1500\n",
      "Configuration saved in ./outputs/checkpoint-1500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2000\n",
      "Configuration saved in ./outputs/checkpoint-2000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-2500\n",
      "Configuration saved in ./outputs/checkpoint-2500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3000\n",
      "Configuration saved in ./outputs/checkpoint-3000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-3500\n",
      "Configuration saved in ./outputs/checkpoint-3500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4000\n",
      "Configuration saved in ./outputs/checkpoint-4000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-4500\n",
      "Configuration saved in ./outputs/checkpoint-4500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5000\n",
      "Configuration saved in ./outputs/checkpoint-5000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-5500\n",
      "Configuration saved in ./outputs/checkpoint-5500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6000\n",
      "Configuration saved in ./outputs/checkpoint-6000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-6500\n",
      "Configuration saved in ./outputs/checkpoint-6500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7000\n",
      "Configuration saved in ./outputs/checkpoint-7000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-7500\n",
      "Configuration saved in ./outputs/checkpoint-7500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8000\n",
      "Configuration saved in ./outputs/checkpoint-8000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-8500\n",
      "Configuration saved in ./outputs/checkpoint-8500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9000\n",
      "Configuration saved in ./outputs/checkpoint-9000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-9500\n",
      "Configuration saved in ./outputs/checkpoint-9500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10000\n",
      "Configuration saved in ./outputs/checkpoint-10000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-10500\n",
      "Configuration saved in ./outputs/checkpoint-10500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to ./outputs/checkpoint-11000\n",
      "Configuration saved in ./outputs/checkpoint-11000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-11000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/config.json\n",
      "Model weights saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ElectraTokenizerFast, ElectraForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "def read_file(fname: str, correct_labels=False) -> pd.DataFrame:\n",
    "    \"\"\"Reads a filename, return df with text and labels.\n",
    "\n",
    "    Args:\n",
    "        fname (str): Filename to read\n",
    "        correct_labels (bool, optional): If True, offensive instances get labeled 1\n",
    "        and acceptable speech gets labeled 0. Else the labels remain unchanged.\n",
    "        Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: resulting dataframe with columns: text, labels\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_table(fname, sep=\"\\t\", header=None, names=\"text,labels,role\".split(\",\"))\n",
    "    if correct_labels:\n",
    "        offensive_ids = df.labels != \"Acceptable speech\"\n",
    "        df.labels[offensive_ids] = 1\n",
    "        df.labels[~offensive_ids] = 0\n",
    "        df[\"labels\"] = df.labels.astype(int)\n",
    "    df = df.drop(columns=[\"role\"])\n",
    "    return df\n",
    "\n",
    "en_test, en_train = \"../data/merged-en.test.tsv\" , \"../data/merged-en.train.tsv\"\n",
    "hr_test, hr_train = \"../data/merged-hr.test.tsv\" , \"../data/merged-hr.train.tsv\"\n",
    "sl_test, sl_train = \"../data/merged-sl.test.tsv\",  \"../data/merged-sl.train.tsv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_df = read_file(hr_train, correct_labels=True)\n",
    "test_df = read_file(hr_test, correct_labels=True)\n",
    "\n",
    "train_texts, train_labels = train_df.text.values.tolist(), train_df.labels.values.tolist()\n",
    "test_texts, test_labels = test_df.text.values.tolist(), test_df.labels.values.tolist()\n",
    "\n",
    "class MergedHateDataset(Dataset):\n",
    "    \"\"\" A dataset class for the merged hatespeech dataset (Frank)\n",
    "    \"\"\"    \n",
    "    def __init__(self, encodings, labels) -> None:\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[index])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "initial_filename = \"EMBEDDIA/crosloengual-bert\"\n",
    "in_filename = initial_filename\n",
    "out_filename_after_additional_training = \"finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_second_attempt\"\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    if i == 0:\n",
    "        model_name = initial_filename\n",
    "    else:\n",
    "        model_name = in_filename\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "    train_dataset = MergedHateDataset(train_encodings, train_labels)\n",
    "    test_dataset = MergedHateDataset(test_encodings, test_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = \"./outputs\",\n",
    "        num_train_epochs = 5,\n",
    "        per_device_train_batch_size = 4,\n",
    "        warmup_steps = 100,\n",
    "        learning_rate = 3e-5,\n",
    "        logging_dir = \"./runs\",\n",
    "        overwrite_output_dir=True\n",
    "    )\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = test_dataset\n",
    "\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(out_filename_after_additional_training)\n",
    "    tokenizer.save_pretrained(out_filename_after_additional_training)\n",
    "    \n",
    "    os.system(\"for file in {~/macocu/task1/task2/runs,~/macocu/task1/task2/outputs,~/macocu/task1/task2/wandb,~/macocu/task1/task2/cache_dir,}; do rm -rf $file; done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated run with different choice of tokenizers and models imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/vocab.txt from cache at /home/peterr/.cache/huggingface/transformers/09f73beefae6e0412fcfe3cc0d7cdd26efc944b005bfc2a9578ddbe1236ff2b5.32b339c0808458d0322a520d66b9be44f71818893fd19ec5a2e21e19799521ed\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/tokenizer_config.json from cache at /home/peterr/.cache/huggingface/transformers/6df9a50970eafe70edbc4e0098fbe8781b666e868e9b9b36ceb9890bb7fcf0bf.268a1e35e9054a0c7fb4bc185891f11122ab5e15883e3333534cdf3d76681112\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/config.json from cache at /home/peterr/.cache/huggingface/transformers/dbc299fc4baf62000b5932d0e8358b2438cc8db0536056ca6ba6d07d6d484599.62f1d55e869204a000e86539f3cf99f1ea413d915c08dba36e27842d16d08c2d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49601\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBEDDIA/crosloengual-bert/resolve/main/pytorch_model.bin from cache at /home/peterr/.cache/huggingface/transformers/5eb6f38d605ba2a329b257215c4d2ca7f74914bc1f224ffd4f01bc005e47bd28.e1011c59a7da068c6b0be79e739ecac88c0e551e50f89d8025654e94ef118633\n",
      "All model checkpoint weights were used when initializing BertForPreTraining.\n",
      "\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8851\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11065\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-00f45de989b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_filename_after_additional_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_filename_after_additional_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1278\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m             \u001b[0;31m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_outputs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m   1885\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m             \u001b[0minner_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1887\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "def read_file(fname: str, correct_labels=False) -> pd.DataFrame:\n",
    "    \"\"\"Reads a filename, return df with text and labels.\n",
    "\n",
    "    Args:\n",
    "        fname (str): Filename to read\n",
    "        correct_labels (bool, optional): If True, offensive instances get labeled 1\n",
    "        and acceptable speech gets labeled 0. Else the labels remain unchanged.\n",
    "        Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: resulting dataframe with columns: text, labels\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_table(fname, sep=\"\\t\", header=None, names=\"text,labels,role\".split(\",\"))\n",
    "    if correct_labels:\n",
    "        offensive_ids = df.labels != \"Acceptable speech\"\n",
    "        df.labels[offensive_ids] = 1\n",
    "        df.labels[~offensive_ids] = 0\n",
    "        df[\"labels\"] = df.labels.astype(int)\n",
    "    df = df.drop(columns=[\"role\"])\n",
    "    return df\n",
    "\n",
    "en_test, en_train = \"../data/merged-en.test.tsv\" , \"../data/merged-en.train.tsv\"\n",
    "hr_test, hr_train = \"../data/merged-hr.test.tsv\" , \"../data/merged-hr.train.tsv\"\n",
    "sl_test, sl_train = \"../data/merged-sl.test.tsv\",  \"../data/merged-sl.train.tsv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_df = read_file(hr_train, correct_labels=True)\n",
    "test_df = read_file(hr_test, correct_labels=True)\n",
    "\n",
    "train_texts, train_labels = train_df.text.values.tolist(), train_df.labels.values.tolist()\n",
    "test_texts, test_labels = test_df.text.values.tolist(), test_df.labels.values.tolist()\n",
    "\n",
    "class MergedHateDataset(Dataset):\n",
    "    \"\"\" A dataset class for the merged hatespeech dataset (Frank)\n",
    "    \"\"\"    \n",
    "    def __init__(self, encodings, labels) -> None:\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[index])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "initial_filename = \"EMBEDDIA/crosloengual-bert\"\n",
    "in_filename = initial_filename\n",
    "out_filename_after_additional_training = \"finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_third_attempt\"\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    if i == 0:\n",
    "        model_name = initial_filename\n",
    "    else:\n",
    "        model_name = in_filename\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "    train_dataset = MergedHateDataset(train_encodings, train_labels)\n",
    "    test_dataset = MergedHateDataset(test_encodings, test_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = \"./outputs\",\n",
    "        num_train_epochs = 5,\n",
    "        per_device_train_batch_size = 4,\n",
    "        warmup_steps = 100,\n",
    "        learning_rate = 3e-5,\n",
    "        logging_dir = \"./runs\",\n",
    "        overwrite_output_dir=True\n",
    "    )\n",
    "\n",
    "    model = AutoModelForPreTraining.from_pretrained(model_name)\n",
    "    model.overwrite_output_dir = True\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = test_dataset\n",
    "\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(out_filename_after_additional_training)\n",
    "    tokenizer.save_pretrained(out_filename_after_additional_training)\n",
    "    \n",
    "    os.system(\"for file in {~/macocu/task1/task2/runs,~/macocu/task1/task2/outputs,~/macocu/task1/task2/wandb,~/macocu/task1/task2/cache_dir,}; do rm -rf $file; done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-22c4da8313ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pwd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "def read_file(fname: str, correct_labels=False) -> pd.DataFrame:\n",
    "    \"\"\"Reads a filename, return df with text and labels.\n",
    "\n",
    "    Args:\n",
    "        fname (str): Filename to read\n",
    "        correct_labels (bool, optional): If True, offensive instances get labeled 1\n",
    "        and acceptable speech gets labeled 0. Else the labels remain unchanged.\n",
    "        Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: resulting dataframe with columns: text, labels\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_table(fname, sep=\"\\t\", header=None, names=\"text,labels,role\".split(\",\"))\n",
    "    if correct_labels:\n",
    "        offensive_ids = df.labels != \"Acceptable speech\"\n",
    "        df.labels[offensive_ids] = 1\n",
    "        df.labels[~offensive_ids] = 0\n",
    "        df[\"labels\"] = df.labels.astype(int)\n",
    "    df = df.drop(columns=[\"role\"])\n",
    "    return df\n",
    "\n",
    "en_test, en_train = \"../data/merged-en.test.tsv\" , \"../data/merged-en.train.tsv\"\n",
    "hr_test, hr_train = \"../data/merged-hr.test.tsv\" , \"../data/merged-hr.train.tsv\"\n",
    "sl_test, sl_train = \"../data/merged-sl.test.tsv\",  \"../data/merged-sl.train.tsv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_df = read_file(hr_train, correct_labels=True)\n",
    "test_df = read_file(hr_test, correct_labels=True)\n",
    "\n",
    "train_texts, train_labels = train_df.text.values.tolist(), train_df.labels.values.tolist()\n",
    "test_texts, test_labels = test_df.text.values.tolist(), test_df.labels.values.tolist()\n",
    "\n",
    "class MergedHateDataset(Dataset):\n",
    "    \"\"\" A dataset class for the merged hatespeech dataset (Frank)\n",
    "    \"\"\"    \n",
    "    def __init__(self, encodings, labels) -> None:\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[index])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "initial_filename = \"EMBEDDIA/crosloengual-bert\"\n",
    "in_filename = initial_filename\n",
    "out_filename_after_additional_training = \"finetuned_models/HR_hate___EMBEDDIA/crosloengual-bert_5_third_attempt\"\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    if i == 0:\n",
    "        model_name = initial_filename\n",
    "    else:\n",
    "        model_name = in_filename\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "    train_dataset = MergedHateDataset(train_encodings, train_labels)\n",
    "    test_dataset = MergedHateDataset(test_encodings, test_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = \"./outputs\",\n",
    "        num_train_epochs = 5,\n",
    "        per_device_train_batch_size = 4,\n",
    "        warmup_steps = 100,\n",
    "        learning_rate = 3e-5,\n",
    "        logging_dir = \"./runs\",\n",
    "        overwrite_output_dir=True\n",
    "    )\n",
    "\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "    model.overwrite_output_dir = True\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = test_dataset\n",
    "\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(out_filename_after_additional_training)\n",
    "    tokenizer.save_pretrained(out_filename_after_additional_training)\n",
    "    \n",
    "    os.system(\"for file in {~/macocu/task1/task2/runs,~/macocu/task1/task2/outputs,~/macocu/task1/task2/wandb,~/macocu/task1/task2/cache_dir,}; do rm -rf $file; done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
